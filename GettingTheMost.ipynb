{
 "metadata": {
  "name": "",
  "signature": "sha256:e3aece1331e5a5819f863477b84e690e429b9cd822becdc6598504cad068e1b9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Getting the Most Out Of VW\n",
      "\n",
      "`vw` has *lots* of command-line arguments. For some of them you have to learn a little bit about how `vw` works internally. This notebook assume that you've made your way successfully through the [Getting Started](GettingStarted.ipynb) notebook first. In this notebook we'll make our way through the following topics:\n",
      "\n",
      "* [Adjusting the number of bits used to store models](#capacity)\n",
      "* [Using some NLP-style feature extractors](#capacity) ([word affixes](#affixes), [spelling](#spelling) and [n-grams](#ngram))\n",
      "* [Changing the loss function that's being optimized](#loss) and [probabilistic output](#prob)\n",
      "* [Getting a human-readable model out of `vw`](#human)\n",
      "* [Changing the default holdout settings (eg to use NLP-style \"development data\")](#holdout)\n",
      "* [Namespaces](#ns) and [quadratic features](#quad)\n",
      "* [Regularization](#reg)\n",
      "* [Neural networks](#nn)\n",
      "* [Summary](#summary)\n",
      "\n",
      "# <a id='capacity'></a> Increasing Representational Capacity (and memory usage)\n",
      "\n",
      "Let's start with our previous training example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 18\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      740\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      630\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      870\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      526\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      490\r\n",
        "0.562500 0.437500           32           32.0  -1.0000   1.0000      454\r\n",
        "0.515625 0.468750           64           64.0  -1.0000   1.0000      520\r\n",
        "0.398438 0.281250          128          128.0   1.0000   1.0000      563\r\n",
        "0.382812 0.367188          256          256.0   1.0000   1.0000     1311\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.357422 0.332031          512          512.0   1.0000   1.0000      387\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.312500 0.267578         1024         1024.0   1.0000  -1.0000      466\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.202643 0.202643         2048         2048.0  -1.0000  -1.0000      578 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.182418 0.162281         4096         4096.0   1.0000   1.0000      331 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.169231 0.156044         8192         8192.0   1.0000   1.0000      955 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 9\r\n",
        "weighted example sum = 12960.000000\r\n",
        "weighted label sum = -54.000000\r\n",
        "average loss = 0.143750 h\r\n",
        "best constant = -0.004167\r\n",
        "best constant's loss = 0.999983\r\n",
        "total feature number = 9754173\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Internally, in order to be fast, `vw` never stores any strings. When it reads your input (in which your features were represented as strings!), it *immediately* hashes the strings to some feature index. By default, it uses $2^{18}$ possible feature indices; this magic number $18$ is the \"number of bits\" used to store the weights in the model. (This is something of a misnomer: it's really the number of parameters in the learning algorithm, which is roughly the number of floats.)\n",
      "\n",
      "What does this hashing accomplish? Well, it accomplishes speed because no string manipulation ever happens. However, it comes with two downsides. The first is that you can get hash collisions. In particular, you might have to different features (i.e., different strings) that hash to the same location. From the learning algorithm's perspective, this means these two features are indistinguishable.\n",
      "\n",
      "Remember that hash collisions are incredibly common. In NLP land, we often have several hundred thousand unique features. By a simple birthday-paradox type argument, we know that the probability of collision when you have $k$ items into $N$ buckets is approximately $1-\\exp\\left[\\frac {k(k-1)} {2N}\\right]$. In this case, with $N=2^{18}$, even with only $2000$ unique features, the probability of collision is already 99.95%. With $100k$ features it's basically guaranteed.\n",
      "\n",
      "The solution is to increase the number of bits used in the representation. This will (a) reduce the number of collisions, (b) make `vw` take more RAM, (c) make `vw` somewhat slower, and (d) make the resulting models larger on disk. Currently, the maximum number of bits that `vw` will let you use is 31. I don't suggest using this; it means `vw` will consume about 8GB of memory while running and the resulting file may take as much as 2GB of disk space. [Runtime memory will be 4 times larger than disk space because the optimization algorithms need extra working memory.] But note that with 100k unique features, even with 31 bits, the probability of collision is 99.1%. It will happen.\n",
      "\n",
      "In practice I usually use around 24 bits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      740\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      630\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      870\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      526\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      490\r\n",
        "0.531250 0.375000           32           32.0  -1.0000   1.0000      454\r\n",
        "0.500000 0.468750           64           64.0  -1.0000   1.0000      520\r\n",
        "0.398438 0.296875          128          128.0   1.0000   1.0000      563\r\n",
        "0.386719 0.375000          256          256.0   1.0000   1.0000     1311\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.355469 0.324219          512          512.0   1.0000   1.0000      387\r\n",
        "0.312500 0.269531         1024         1024.0   1.0000  -1.0000      466\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.193833 0.193833         2048         2048.0  -1.0000  -1.0000      578 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.178022 0.162281         4096         4096.0   1.0000   1.0000      331 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.164835 0.151648         8192         8192.0   1.0000   1.0000      955 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 9\r\n",
        "weighted example sum = 12960.000000\r\n",
        "weighted label sum = -54.000000\r\n",
        "average loss = 0.143750 h\r\n",
        "best constant = -0.004167\r\n",
        "best constant's loss = 0.999983\r\n",
        "total feature number = 9754173\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, increasing the number of bits did not help test accuracy, but we will see that, when we add additional features, it becomes more important.\n",
      "\n",
      "# <a id='nlp'></a>  Fun NLP-esque Features for Free\n",
      "\n",
      "One nice thing about `vw` is that it internally supports \"extra feature\" generation. The main useful features are: word prefixes and suffixes, spelling features, and ngram features.\n",
      "\n",
      "## <a id='affixes'></a> Word Affixes\n",
      "\n",
      "For NLP tasks that mostly depend on the *meaning* (aka \"semantics\") of words, we often don't care about the funny little things that come at the ends of words. For instance, for sentiment classification, the words `awesome` and `awesomeness` are likely to roughly mean the same thing. For other tasks, like part of speech tagging, it's the suffixes that might matter most: words that end in `-ness` are much more likely to be adjectives than anything else.\n",
      "\n",
      "`vw` can automatically generate word prefixes and suffixes for you, using the `--affix` feature. For instance, if you add \"`--affix +5,-3`\" to the command line, this says to automatically compute (and add as new features) 5-character prefixes (that's the `+`) and three character suffixes (that's the `-`).\n",
      "\n",
      "Let's try it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --affix +6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     1479\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1259\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     1739\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1051\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      979\r\n",
        "0.562500 0.437500           32           32.0  -1.0000   1.0000      907\r\n",
        "0.515625 0.468750           64           64.0  -1.0000   1.0000     1039\r\n",
        "0.421875 0.328125          128          128.0   1.0000   1.0000     1125\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.414062 0.406250          256          256.0   1.0000   1.0000     2621\r\n",
        "0.363281 0.312500          512          512.0   1.0000   1.0000      773\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.319336 0.275391         1024         1024.0   1.0000  -1.0000      931\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.211454 0.211454         2048         2048.0  -1.0000  -1.0000     1155 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.189011 0.166667         4096         4096.0   1.0000   1.0000      661 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.168132 0.147253         8192         8192.0   1.0000   1.0000     1909 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 7\r\n",
        "weighted example sum = 10080.000000\r\n",
        "weighted label sum = -42.000000\r\n",
        "average loss = 0.137500 h\r\n",
        "best constant = -0.004167\r\n",
        "best constant's loss = 0.999983\r\n",
        "total feature number = 15163078\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That was (somewhat) helpful -- the holdout loss dropped from 14.4% to 13.8%. Perhaps not an ACL paper, but at least it's doing something! Note, satisfyingly, that the number of features approximately doubled. (In fact, when the old feature count was 955, the new feature count is 1909. This is because $1909=1 + 954 * 2$. The 955 old features includes 954 words and a bias feature. Each word gets an affix so we have $954 * 2 = 1908$ real features, plus a bias.)\n",
      "\n",
      "## <a id='spelling'></a> Spelling Features\n",
      "\n",
      "Spelling features are *super* useful for tasks where things like capitalization, years, numbers, etc. matter a lot. In other words, tasks *not at all* like sentiment classification.\n",
      "\n",
      "In `vw`, the spelling features option tells it to generate new features based on the word forms seen. For example, a word \"Alice\" has the word form \"Aaaaa\" (meaning: a capital letter followed by four lowercase letters); \"VanBuren\" has the form \"AaaAaaaa\". The general rule is that digits 0-9 get mapped to \"0\", letters a-z to \"a\", letters A-Z to \"A\", period to \".\" and anything else to \"#\". Thus, \"xY9s,3.80vaq\" gets mapped to \"aA0a#0.00aaa\" and this new \"word form\" is used as a new feature.\n",
      "\n",
      "To turn on spelling features, you simply add `--spelling _` to the command line. We can do this with little expectation that it will help:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --spelling _"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     1479\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1259\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     1739\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1051\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      979\r\n",
        "0.593750 0.500000           32           32.0  -1.0000   1.0000      907\r\n",
        "0.500000 0.406250           64           64.0  -1.0000   1.0000     1039\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.468750 0.437500          128          128.0   1.0000  -1.0000     1125\r\n",
        "0.472656 0.476562          256          256.0   1.0000   1.0000     2621\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.482422 0.492188          512          512.0   1.0000  -1.0000      773\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.427734 0.373047         1024         1024.0   1.0000  -1.0000      931\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.286344 0.286344         2048         2048.0  -1.0000  -1.0000     1155 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.232967 0.179825         4096         4096.0   1.0000   1.0000      661 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.194505 0.156044         8192         8192.0   1.0000   1.0000     1909 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 8\r\n",
        "weighted example sum = 11520.000000\r\n",
        "weighted label sum = -48.000000\r\n",
        "average loss = 0.150000 h\r\n",
        "best constant = -0.004167\r\n",
        "best constant's loss = 0.999983\r\n",
        "total feature number = 17329232\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nope, indeed it did not help and it actually hurt slightly (14.4% error to 15.0% error).\n",
      "\n",
      "You might wonder what the \"`_`\" in the command line means; for now, don't worry about it. We'll come back to this when we talk about namespaces.\n",
      "\n",
      "## <a id='ngram'></a> N-gram Features\n",
      "\n",
      "Our current representation for learning is bag of words. Some times looking at a single word at a time is insufficient and we want to, instead, look at (contiguous) sequences of words: ngrams.\n",
      "\n",
      "Given an example text \"the monster ate a sandwich\", if we were to augment this with bigram features, we would get \"the_monster monster_ate ate_a a_sandwich\". In cases where word bigrams are useful, this is going to be helpful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Generating 2-grams for all namespaces.\r\n",
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     1478\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1258\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     1738\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1050\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      978\r\n",
        "0.531250 0.375000           32           32.0  -1.0000   1.0000      906\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.453125 0.375000           64           64.0  -1.0000  -1.0000     1038\r\n",
        "0.398438 0.343750          128          128.0   1.0000   1.0000     1124\r\n",
        "0.386719 0.375000          256          256.0   1.0000   1.0000     2620\r\n",
        "0.330078 0.273438          512          512.0   1.0000   1.0000      772\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.276367 0.222656         1024         1024.0   1.0000  -1.0000      930\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.154185 0.154185         2048         2048.0  -1.0000  -1.0000     1154 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.140659 0.127193         4096         4096.0   1.0000   1.0000      660 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.134066 0.127473         8192         8192.0   1.0000   1.0000     1908 h\r\n",
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 6\r\n",
        "weighted example sum = 8640.000000\r\n",
        "weighted label sum = -36.000000\r\n",
        "average loss = 0.118750 h\r\n",
        "best constant = -0.004167\r\n",
        "best constant's loss = 0.999983\r\n",
        "total feature number = 12988284\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow, that was super useful! Loss dropped from 14.4% to 11.9%! We can try trigram features too:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Generating 3-grams for all namespaces.\r\n",
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     2215\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1885\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     2605\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1573\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000     1465\r\n",
        "0.531250 0.375000           32           32.0  -1.0000   1.0000     1357\r\n",
        "0.468750 0.406250           64           64.0  -1.0000  -1.0000     1555\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.375000 0.281250          128          128.0   1.0000   1.0000     1684\r\n",
        "0.351562 0.328125          256          256.0   1.0000   1.0000     3928\r\n",
        "0.316406 0.281250          512          512.0   1.0000   1.0000     1156\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.275391 0.234375         1024         1024.0   1.0000   1.0000     1393\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.158590 0.158590         2048         2048.0  -1.0000  -1.0000     1729 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.145055 0.131579         4096         4096.0   1.0000   1.0000      988 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.137363 0.129670         8192         8192.0   1.0000   1.0000     2860 h\r\n",
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 6\r\n",
        "weighted example sum = 8640.000000\r\n",
        "weighted label sum = -36.000000\r\n",
        "average loss = 0.118750 h\r\n",
        "best constant = -0.004167\r\n",
        "best constant's loss = 0.999983\r\n",
        "total feature number = 19465146\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, that didn't help any more.\n",
      "\n",
      "We can, however, now see that the number of bits matters. If we drop the number of bits back down to 18, we get:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 18 --ngram 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Generating 3-grams for all namespaces.\r\n",
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 18\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     2215\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1885\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     2605\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1573\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000     1465\r\n",
        "0.531250 0.375000           32           32.0  -1.0000   1.0000     1357\r\n",
        "0.437500 0.343750           64           64.0  -1.0000  -1.0000     1555\r\n",
        "0.375000 0.312500          128          128.0   1.0000   1.0000     1684\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.355469 0.335938          256          256.0   1.0000   1.0000     3928\r\n",
        "0.320312 0.285156          512          512.0   1.0000   1.0000     1156\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.284180 0.248047         1024         1024.0   1.0000   1.0000     1393\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.158590 0.158590         2048         2048.0  -1.0000  -1.0000     1729 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.156044 0.153509         4096         4096.0   1.0000   1.0000      988 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.154945 0.153846         8192         8192.0   1.0000   1.0000     2860 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 8\r\n",
        "weighted example sum = 11520.000000\r\n",
        "weighted label sum = -48.000000\r\n",
        "average loss = 0.143750 h\r\n",
        "best constant = -0.004167\r\n",
        "best constant's loss = 0.999983\r\n",
        "total feature number = 25953528\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is no better than we started with.\n",
      "\n",
      "More specifically: **if we hadn't increased the number of bits, we would have concluded that ngram features weren't useful!** This is why I always use as many bits as I can tolerate (sometimes up to 27 or 29).\n",
      "\n",
      "Finally, `vw` can do \"skip ngrams\" too. This means that instead of only looking at bigrams of adjacent words, you can look at bigrams with some gap. For instance, if you say `--ngram 2 --skips 1`, this means \"compute all bigrams that have at most one gap in them.\" For our favorite sentence \"the monster ate a sandwich\", you would get the default bigram features (\"the_monster monster_ate ate_a a_sandwich\") and *also* the skips (\"the_ate monster_a ate_sandwich\").\n",
      "\n",
      "Note: as you increase to, say, four-grams, this automatically includes bigrams and trigrams. As you increase number of skips, you get all the lower order skips too.\n",
      "\n",
      "# <a id='loss'></a> Changing the Loss Function\n",
      "\n",
      "By default, `vw` optimizes squared loss. This means that if the correct label for an example is +1, and the model predicts -1, the error is 4.0. However, if the model predicts +3, the error is still 4.0, even though it's making the right binary prediction. Squared loss has the nice property that it estimates means. But it's not necessarily the most natural loss for classification problems.\n",
      "\n",
      "Many people prefer logistic loss (which gives a nice probabilistic interpretation) or hinge loss (which, when combined with regularization, yields support vector machines). You can switch the loss function with a simple command-line flag:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 18\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      740\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      630\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      870\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      526\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      490\r\n",
        "0.468750 0.250000           32           32.0  -1.0000  -1.0000      454\r\n",
        "0.390625 0.312500           64           64.0  -1.0000  -1.0000      520\r\n",
        "0.351562 0.312500          128          128.0   1.0000  -1.0000      563\r\n",
        "0.375000 0.398438          256          256.0   1.0000   1.0000     1311\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.345703 0.316406          512          512.0   1.0000   1.0000      387\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.295898 0.246094         1024         1024.0   1.0000  -1.0000      466\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.198238 0.198238         2048         2048.0  -1.0000  -1.0000      578 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.173626 0.149123         4096         4096.0   1.0000   1.0000      331 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.158242 0.142857         8192         8192.0   1.0000   1.0000      955 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.138462 0.118681        16384        16384.0   1.0000   1.0000      655 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 12\r\n",
        "weighted example sum = 17280.000000\r\n",
        "weighted label sum = -72.000000\r\n",
        "average loss = 0.112500 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 13005564\r\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='prob'></a> If you're using logistic loss, you can get probabilistic predictions out of you model by using `-r`(aw) output; for example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary -i data/sentiment.model -t -r data/sentiment.te.pred data/sentiment.te --quiet\n",
      "!head data/sentiment.te.pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.912703\r\n",
        "4.235941\r\n",
        "-0.865008\r\n",
        "0.956259\r\n",
        "-4.988661\r\n",
        "-5.052917\r\n",
        "-1.975477\r\n",
        "-0.460777\r\n",
        "-5.389775\r\n",
        "5.254548\r\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These predictions are values *before* being hit with a logistic function. To get probabilities, map $z \\mapsto \\frac 1 {1 + \\exp(-z)}$, as in:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head data/sentiment.te.pred | perl -ne '$a = 1/(1+exp(-$_)); print \"$a\\n\";'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.871322511409019\r\n",
        "0.985740095399934\r\n",
        "0.29629409396628\r\n",
        "0.722372169753656\r\n",
        "0.00676865645321188\r\n",
        "0.0063500834086109\r\n",
        "0.121801818717659\r\n",
        "0.386801514057067\r\n",
        "0.0045422735017069\r\n",
        "0.994803438440386\r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can alternatively switch to hinge loss:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function hinge"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 18\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      740\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      630\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      870\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      526\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      490\r\n",
        "0.593750 0.500000           32           32.0  -1.0000   1.0000      454\r\n",
        "0.484375 0.375000           64           64.0  -1.0000   1.0000      520\r\n",
        "0.398438 0.312500          128          128.0   1.0000   1.0000      563\r\n",
        "0.402344 0.406250          256          256.0   1.0000   1.0000     1311\r\n",
        "0.367188 0.332031          512          512.0   1.0000   1.0000      387\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.318359 0.269531         1024         1024.0   1.0000   1.0000      466\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.202643 0.202643         2048         2048.0  -1.0000  -1.0000      578 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.191209 0.179825         4096         4096.0   1.0000   1.0000      331 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.175824 0.160440         8192         8192.0   1.0000   1.0000      955 h\r\n",
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 7\r\n",
        "weighted example sum = 10080.000000\r\n",
        "weighted label sum = -42.000000\r\n",
        "average loss = 0.156250 h\r\n",
        "best constant = -1.000000\r\n",
        "best constant's loss = 0.995833\r\n",
        "total feature number = 7586579\r\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, logistic loss does awesome (dropping the error from 14.4% to 11.3%) and hinge does crummy (increasing to 15.6%). This is perhaps because with hinge we probably want to regularize, which we'll get back to later.\n",
      "\n",
      "# <a id='human'></a> Getting a Human Readable Model\n",
      "\n",
      "Admittedly one of the most annoying things with `vw` is getting a human-readable model out. In some more complex cases, this is nearly impossible. And even in simple cases (like those here), it's cumbersome. This is because of the fact that `vw` doesn't store strings. So if you want to get a mapping from features-to-weights out of `vw` you have to jump through some hoops.\n",
      "\n",
      "What are those hoops?\n",
      "\n",
      "First you have to learn a model and save it to disk. Fine, we know how to do that. You then have to instruct `vw` to load the saved model and take another pass over your training data, and save the results to disk. It has to take another pass over the data because as it makes that final pass, it actually *does* store the strings in memory (if you tell it to) so it can generate the human readable file.\n",
      "\n",
      "What does this look like in practice? First, we generate a model. For a warm-up, we won't use any fancy features. Then, we have to do a second incantation of `vw` and tell it where to store the resulting human-readable model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic --quiet\n",
      "!vw -i data/sentiment.model -t --invert_hash data/sentiment.model.readable data/sentiment.tr --quiet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first command here trains the model as before. The second says: start from that pre-trained model; go into test mode (so that you don't adjust any of the weights of the model); store the resulting readable model (`--invert_hash`) into the specified file; and read from `data/sentiment.tr` (you have to re-read from the same training data).\n",
      "\n",
      "We can now look at `data/sentiment.model.readable` to see what's going on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n40 data/sentiment.model.readable"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Version 8.1.1\r\n",
        "Id\r\n",
        "Min label:-50\r\n",
        "Max label:50\r\n",
        "bits:18\r\n",
        "lda:0\r\n",
        "0 ngram:\r\n",
        "0 skip:\r\n",
        "options:\r\n",
        "Checksum: 295637807\r\n",
        ":0\r\n",
        "\u0005:0:-0.0378245\r\n",
        "\u0013earth:20130:-0.056972\r\n",
        "\u0013if:138654:-0.0740175\r\n",
        "\u0013ripley:100759:0.0392712\r\n",
        "\u0013they:88541:0.00538131\r\n",
        "\u0014:0:-0.0378245\r\n",
        "\u0016:0:-0.0378245\r\n",
        "!:138484:0.0169136\r\n",
        "\":130227:-0.0400175\r\n",
        "#:91983:-0.0140879\r\n",
        "#$%:211190:-0.0395448\r\n",
        "#1:203549:-0.0496337\r\n",
        "#13:61266:-0.0256448\r\n",
        "#15:147795:0.0424534\r\n",
        "#2:96578:0.00258815\r\n",
        "#3:69831:0.00970923\r\n",
        "#6:57800:0.0159506\r\n",
        "#7:163288:0.0296533\r\n",
        "#8:117040:0.0496203\r\n",
        "#9:139064:0.0282622\r\n",
        "#@%$^:82571:0.0245689\r\n",
        "$$$:202613:0.0479335\r\n",
        "$1:42170:0.0542853\r\n",
        "$10:169102:0.00224398\r\n",
        "$100:94241:-0.00705101\r\n",
        "$1000:224416:-0.0164994\r\n",
        "$100m:216311:0.0162872\r\n",
        "$11:197221:-0.0164035\r\n",
        "$110:186637:0.0123556\r\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The beginning of this output is some header information that tells you a bit about the type of model that was stored. After the `options:` line there's the `:0` line, and then after that you get a list of `feature:hash:weight` triples. For instance, the feature \"`earth`\" was hashed to position 20130, and has a feature weight -0.056972, which means it's (very) mildly indicative of the negative class. These words are actually sorted; the reason \"earth\" pops up at the top is because the \"e\" in the string is some weird unicode \"e\" and not the normal ascii \"e\".\n",
      "\n",
      "We can extract just the features by dropping off the first 12 lines of the output, and then sort by the feature weights:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mid-teen:33954:9.41436e-05\r\n",
        "bulb:172735:7.93612e-05\r\n",
        "rigg:172735:7.93612e-05\r\n",
        "culinary:256649:5.93936e-05\r\n",
        "excrutiatingly:226816:5.45747e-05\r\n",
        "horse:126920:4.88297e-05\r\n",
        "convincing:107110:4.67978e-05\r\n",
        "full-time:107110:4.67978e-05\r\n",
        "semi-autobiographical:107110:4.67978e-05\r\n",
        "vultures:42919:4.41443e-06\r\n",
        "sort: write failed: standard output: Broken pipe\r\n",
        "sort: write error\r\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Ignore the broken pipe issues, which are a ipython notebook joy.)\n",
      "\n",
      "Here, we've dropped the first 12 lines, then sorted the remaining by the third column in reverse numerical order (`-k3nr`) where columns are separated by colons (`-t:`) and then looked at the top 10.\n",
      "\n",
      "These are kind of weird features to see at the top of a sentiment classification data set. Presumably this means that people like Hamlet and probably also like Matt Damon. But there's very little more we can get from this.\n",
      "\n",
      "We can look at the most negative features too:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | tail"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "codenamed:126301:-6.10351e-05\r\n",
        "corporation:126301:-6.10351e-05\r\n",
        "misogynistic:59954:-6.40186e-05\r\n",
        "tripplehorn:170418:-7.29749e-06\r\n",
        "conversing:161289:-7.59994e-05\r\n",
        "dictator_:45751:-7.68324e-05\r\n",
        "tongue-in-cheek:45751:-7.68324e-05\r\n",
        "crummy:39842:-7.95944e-05\r\n",
        "mentor:26147:-7.97019e-05\r\n",
        "oven:61859:-9.24248e-05\r\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are much more reasonable, and the magnitudes are significantly larger (if negative), suggesting that these are probably what is really being used to make good predictions.\n",
      "\n",
      "We can do the same thing with the model with ngram features:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic --ngram 3 -b 24 --quiet\n",
      "!vw -i data/sentiment.model -t --invert_hash data/sentiment.model.readable data/sentiment.tr --quiet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You'll notice this took a bit longer to run because there are *lots* of ngrams. We can see the output as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!echo \"Top positive features\"\n",
      "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head\n",
      "!echo \"\"\n",
      "!echo \"Top negative features\"\n",
      "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | tail"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top positive features\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "onboard:3626211:9.99471e-05\r\n",
        "(^will^):12875395:9.9944e-05\r\n",
        "up^throughout:13931562:9.98296e-05\r\n",
        "up^throughout^the:11871460:9.98296e-05\r\n",
        "moderately^successful:6389639:9.98243e-05\r\n",
        "his^big:13485001:9.97835e-05\r\n",
        "it's^about^how:9756040:9.97726e-05\r\n",
        "enough^here:10773601:9.97029e-05\r\n",
        "humans^who:10672839:9.95343e-05\r\n",
        "and^caan^.:13586681:9.93961e-06\r\n",
        "sort: write failed: standard output: Broken pipe\r\n",
        "sort: write error\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top negative features\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "'total^recall'^):14457084:-9.96815e-05"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "a^refreshing^break:6925308:-9.96832e-06\r\n",
        "hands^were:6925308:-9.96832e-06\r\n",
        "film^there:11322576:-9.9757e-05\r\n",
        "works^for^is:10728392:-9.97598e-05\r\n",
        "proceeds^to:15717437:-9.97956e-05\r\n",
        "was^missing:10310295:-9.98513e-05\r\n",
        "his^bachelor:4417000:-9.98532e-05\r\n",
        "his^bachelor^party:13916184:-9.98532e-05\r\n",
        "oddly:5532297:-9.98714e-05\r\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In all of these, there's only one ngram feature that pops up as negative, \"`puppetry^and/or^hate`\". You can google this trigram if you really want to know what it's from. Amusingly, \"attempt\" also comes up as a negative word. I saw this once in a review for one of my papers: \"this paper attempts to do XYZ.\" Not a good sign.\n",
      "\n",
      "# <a id='holdout'></a> Changing `vw`'s Default Holdout Settings\n",
      "\n",
      "If you recall from the introduction, the default way `vw` works for doing multiple passes is: on the first pass, perform progressive validation; on subsequent passes, use every 10th example as a heldout \"validation\" example. And to stop optimizing when things don't improve for three passes.\n",
      "\n",
      "These are reasonable defaults, but somewhat at odds with the behavior I often want.\n",
      "\n",
      "First, I often *don't* want `vw` to do early stopping. If I tell it to do 20 passes, then by golly it should do 20 passes. This is easy. I just say `--early_terminate 999`. This means that instead of needing 3 passes of no-improvement in order to terminate, it now needs 999. Since I never run that many passes, this is a good default to say \"don't stop early.\" However, if *will* still output only the best model found.\n",
      "\n",
      "More relevant, often in NLP we have training data, development data, and test data. And I want to get validation performance on the development data rather than every-10th-example. You can accomplish this with `--holdout_after N`. What this means is: instead of doing every-10th-example as validation, use the first (N-1) examples as training data, and anything after that as development data.\n",
      "\n",
      "Putting these together, I usually do something like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --early_terminate 999 --holdout_after 1401"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "final_regressor = data/sentiment.model\r\n",
        "Num weight bits = 18\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment.tr.cache\r\n",
        "Reading datafile = data/sentiment.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      740\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      630\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      870\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      526\r\n",
        "0.625000 0.750000           16           16.0  -1.0000   1.0000      529\r\n",
        "0.531250 0.437500           32           32.0   1.0000  -1.0000     1188\r\n",
        "0.468750 0.406250           64           64.0  -1.0000   1.0000      931\r\n",
        "0.375000 0.281250          128          128.0   1.0000  -1.0000      662\r\n",
        "0.406250 0.437500          256          256.0   1.0000   1.0000      922\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.343750 0.281250          512          512.0   1.0000  -1.0000      297\r\n",
        "0.307617 0.271484         1024         1024.0   1.0000   1.0000      991\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.245000 0.245000         2048         2048.0   1.0000   1.0000      724 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.230000 0.215000         4096         4096.0   1.0000   1.0000      695 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.229000 0.228333         8192         8192.0  -1.0000  -1.0000      623 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.218636 0.210000        16384        16384.0  -1.0000  -1.0000      496 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1400\r\n",
        "passes used = 20\r\n",
        "weighted example sum = 28000.000000\r\n",
        "weighted label sum = 240.000000\r\n",
        "average loss = 0.190000 h\r\n",
        "best constant = 0.008571\r\n",
        "best constant's loss = 0.999927\r\n",
        "total feature number = 21021180\r\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, the important thing is that the first 1400 examples are used as training data, the the remaining examples (in this case, 200) are used as heldout data. The average loss reported is then *precisely* the average loss on this heldout data.\n",
      "\n",
      "# <a id='ns'></a>  Namespaces and quadratic features\n",
      "\n",
      "For this part of the tutorial to make sense, we have to make our task a little more interesting.\n",
      "\n",
      "Many people who do sentiment analysis start from a *sentiment lexicon*: basically, a list of positive-ish and negative-ish words. There are [lots of sentiment lexicons](http://sentiment.christopherpotts.net/lexicons.html). We will use [the one from Bing Liu](http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). First, let's download it and decompress it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm -f data/*words.txt\n",
      "!curl -o data/opinion-lexicon-English.rar https://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
      "!rar x data/opinion-lexicon-English.rar  data\n",
      "!ls -l data/*words.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "100 23342  100 23342    0     0   115k      0 --:--:-- --:--:-- --:--:--  115k\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "RAR 4.20   Copyright (c) 1993-2012 Alexander Roshal   9 Jun 2012\r\n",
        "Trial version             Type RAR -? for help\r\n",
        "\r\n",
        "\r\n",
        "Extracting from data/opinion-lexicon-English.rar\r\n",
        "\r\n",
        "Extracting  data/negative-words.txt                                      \b\b\b\b 67%\b\b\b\b\b  OK \r\n",
        "Extracting  data/positive-words.txt                                      \b\b\b\b 99%\b\b\b\b\b  OK \r\n",
        "All OK\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-rw-r--r-- 1 hal hal 46299 Mar 12  2011 data/negative-words.txt\r\n",
        "-rw-r--r-- 1 hal hal 20630 Mar 12  2011 data/positive-words.txt\r\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can look at some of the positive and negative words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n50 data/*-words.txt | grep -v '^;'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "==> data/negative-words.txt <==\r\n",
        "\r\n",
        "2-faced\r\n",
        "2-faces\r\n",
        "abnormal\r\n",
        "abolish\r\n",
        "abominable\r\n",
        "abominably\r\n",
        "abominate\r\n",
        "abomination\r\n",
        "abort\r\n",
        "aborted\r\n",
        "aborts\r\n",
        "abrade\r\n",
        "abrasive\r\n",
        "abrupt\r\n",
        "abruptly\r\n",
        "\r\n",
        "==> data/positive-words.txt <==\r\n",
        "\r\n",
        "a+\r\n",
        "abound\r\n",
        "abounds\r\n",
        "abundance\r\n",
        "abundant\r\n",
        "accessable\r\n",
        "accessible\r\n",
        "acclaim\r\n",
        "acclaimed\r\n",
        "acclamation\r\n",
        "accolade\r\n",
        "accolades\r\n",
        "accommodative\r\n",
        "accomodative\r\n",
        "accomplish\r\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this, I dropped lines that begin with \"`;`\" because these are comments in the files.\n",
      "\n",
      "We now want to go back and generate some new data files for `vw` that include lexicon features. In particular, we will include *both* the bag of words representation *as well as* lexicon features. The lexicon features we will use are very simple: the log of the count of words in the document that are on the positive list, and the log of the count on the negative list. We use logs because getting more positive words has diminishing returns.\n",
      "\n",
      "To do this, we'll write a bit more python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadLexicon(filename):\n",
      "    with open(filename, 'r') as h:\n",
      "        return set([l.strip() \n",
      "                    for l in h.readlines()\n",
      "                    if  not l.startswith(';') and len(l) > 1])\n",
      "\n",
      "import math\n",
      "def countLexiconWords(text, lexicon):\n",
      "    return math.log(1.0 + len([w for w in text if w in lexicon]))\n",
      "\n",
      "positiveLexicon = loadLexicon('data/positive-words.txt')\n",
      "negativeLexicon = loadLexicon('data/negative-words.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have a copy of the two lexicons and we want to generate `vw` examples.\n",
      "\n",
      "But we have two different types of features. We have the original bag of words features. And we have these lexicon features. We'd like to keep them separate.\n",
      "\n",
      "This is where feature namespaces come in. We're going to create examples with *two* namespaces, one for the bag of words (let's call it the `w` namespace) and one for the lexicon features (let's call that the `l` namespace). In `vw`, namespaces are separated by pipes, so an example might look like:\n",
      "\n",
      "    +1 |l pos:5 neg:2 |w some words might go here ...\n",
      "    \n",
      "In addition to having two namespaces, this example also shows how to use feature values. By default, all features in a `vw` example get a value of one. If you want to override this, you can say something like \"`pos:5`\", which means that there's a single feature (called \"`pos`\") that has a feature value of 5.\n",
      "\n",
      "Let's generate data like this. Some of the code is copied from the Getting Started tutorial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def textToVW(lines):\n",
      "    return ' '.join([l.strip() for l in lines]).replace(':','COLON').replace('|','PIPE')\n",
      "\n",
      "def fileToVW(inputFile, posLex, negLex):\n",
      "    text     = textToVW(open(inputFile,'r').readlines())\n",
      "    words    = text.split()\n",
      "    posCount = countLexiconWords(words, posLex)\n",
      "    negCount = countLexiconWords(words, negLex)\n",
      "    return '|l pos:%g neg:%g |w ' % (posCount,negCount) + text\n",
      "\n",
      "import os\n",
      "def readTextFilesInDirectory(directory):\n",
      "    return [fileToVW(directory + os.sep + f, positiveLexicon, negativeLexicon) \n",
      "            for f in os.listdir(directory)\n",
      "            if  f.endswith('.txt')]\n",
      "\n",
      "examples = ['+1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/pos')] + \\\n",
      "           ['-1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/neg')]\n",
      "\n",
      "print '%d total examples read' % len(examples)\n",
      "print 'first example: %s...' % examples[ 0][:70]\n",
      "print 'last  example: %s...' % examples[-1][:70]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2000 total examples read\n",
        "first example: +1 |l pos:4.54329 neg:3.4012 |w note COLON some may consider portions ...\n",
        "last  example: -1 |l pos:2.63906 neg:3.4012 |w while i am not fond of any writer's us...\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At least based on these two examples, this seems promising: the positive/negative lexicon features seem to correlate with the labels!\n",
      "\n",
      "Let's generate a new `vw` training file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "random.seed(1234)\n",
      "random.shuffle(examples)   # this does in-place shuffling\n",
      "\n",
      "def writeToVWFile(filename, examples):\n",
      "    with open(filename, 'w') as h:\n",
      "        for ex in examples:\n",
      "            print >>h, ex\n",
      "            \n",
      "writeToVWFile('data/sentiment-lex.tr', examples[:1600])\n",
      "writeToVWFile('data/sentiment-lex.te', examples[1600:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we're in a position where we can train a model. Let's use exactly the same command as earlier, which got us 11.9% error rate, but using the new data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      742\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      632\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      872\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      528\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      492\r\n",
        "0.468750 0.250000           32           32.0  -1.0000  -1.0000      456\r\n",
        "0.390625 0.312500           64           64.0  -1.0000  -1.0000      522\r\n",
        "0.351562 0.312500          128          128.0   1.0000  -1.0000      565\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.375000 0.398438          256          256.0   1.0000   1.0000     1313\r\n",
        "0.347656 0.320312          512          512.0   1.0000   1.0000      389\r\n",
        "0.296875 0.246094         1024         1024.0   1.0000  -1.0000      468\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.193833 0.193833         2048         2048.0  -1.0000  -1.0000      580 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.171429 0.149123         4096         4096.0   1.0000   1.0000      333 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.158242 0.145055         8192         8192.0   1.0000   1.0000      957 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 11\r\n",
        "weighted example sum = 15840.000000\r\n",
        "weighted label sum = -66.000000\r\n",
        "average loss = 0.118750 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 11953436\r\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And, disappointingly, we still get 11.9% error rate!\n",
      "\n",
      "One thing we can do that is useful is *turn off* a subset of the namespaces. For instance, if we want to *only* use the lexicon features, we can tell `vw` to ignore the `w` namespace:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 --ignore w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ignoring namespaces beginning with: w \r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000        3\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000        3\r\n",
        "0.500000 0.500000            4            4.0   1.0000   1.0000        3\r\n",
        "0.625000 0.750000            8            8.0  -1.0000   1.0000        3\r\n",
        "0.687500 0.750000           16           16.0   1.0000  -1.0000        3\r\n",
        "0.656250 0.625000           32           32.0  -1.0000   1.0000        3\r\n",
        "0.546875 0.437500           64           64.0  -1.0000  -1.0000        3\r\n",
        "0.539062 0.531250          128          128.0   1.0000  -1.0000        3\r\n",
        "0.527344 0.515625          256          256.0   1.0000   1.0000        3\r\n",
        "0.498047 0.468750          512          512.0   1.0000  -1.0000        3\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.463867 0.429688         1024         1024.0   1.0000   1.0000        3\r\n",
        "0.343612 0.343612         2048         2048.0  -1.0000   1.0000        3 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.340659 0.337719         4096         4096.0   1.0000  -1.0000        3 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.332967 0.325275         8192         8192.0   1.0000   1.0000        3 h\r\n",
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 6\r\n",
        "weighted example sum = 8640.000000\r\n",
        "weighted label sum = -36.000000\r\n",
        "average loss = 0.318750 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 25914\r\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Overall that's not too impressive: 31.9% error. On the other hand, this is just using two features (plus a bias).\n",
      "\n",
      "## <a id='quad'></a> Quadratic features\n",
      "\n",
      "The real magic comes from *feature combination*. For instance, the first example from above looks like:\n",
      "\n",
      "    +1 |l pos:4.54329 neg:3.4012 |w note COLON some may consider portions ...\n",
      "    \n",
      "There might be reason to believe that looking at *pairs* of features between the `l` and `w` namespaces would be useful. In this case, these features would be things like:\n",
      "\n",
      "    note_pos:4.5 note_neg:3.4 COLON_pos:4.5 COLON_neg:3.4 some_pos:4.5 ...\n",
      "    \n",
      "(I've rounded 4.54329 to 4.5 and 3.4012 to 3.4 for brevity.)\n",
      "\n",
      "This allows you to model interactions among features. `vw` will do this automatically for you with `-q` (quadratic) features. For example, you can ask for all pairs of features between these two namespaces as:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating quadratic features for pairs: wl \r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     2220\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1890\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     2610\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1578\r\n",
        "0.625000 0.750000           16           16.0   1.0000   1.0000     1470\r\n",
        "0.437500 0.250000           32           32.0  -1.0000  -1.0000     1362\r\n",
        "0.390625 0.343750           64           64.0  -1.0000  -1.0000     1560\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.367188 0.343750          128          128.0   1.0000  -1.0000     1689\r\n",
        "0.347656 0.328125          256          256.0   1.0000   1.0000     3933\r\n",
        "0.324219 0.300781          512          512.0   1.0000   1.0000     1161\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.276367 0.228516         1024         1024.0   1.0000   1.0000     1398\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.180617 0.180617         2048         2048.0  -1.0000  -1.0000     1734 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.153846 0.127193         4096         4096.0   1.0000   1.0000      993 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.137363 0.120879         8192         8192.0   1.0000   1.0000     2865 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 9\r\n",
        "weighted example sum = 12960.000000\r\n",
        "weighted label sum = -54.000000\r\n",
        "average loss = 0.112500 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 29260062\r\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And with that, our loss has dropped from 11.9% to 11.3%. Not a huge win, but something. Note that the number of features per example has approximately tripled here.\n",
      "\n",
      "You can go crazy if you want and add quadratic features between the `l` namespace and itself and the `w` namespace and itself too:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl -q ll -q ww"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating quadratic features for pairs: wl ll ww \r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.000000 1.000000            1            1.0   1.0000  -1.0000   275653\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000   200028\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000   380628\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.625000 0.500000            8            8.0  -1.0000   1.0000   139656\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.750000 0.875000           16           16.0   1.0000  -1.0000   121278\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.593750 0.437500           32           32.0  -1.0000   1.0000   104196\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.437500 0.281250           64           64.0  -1.0000   1.0000   136503\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.359375 0.281250          128          128.0   1.0000   1.0000   159895\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.367188 0.375000          256          256.0   1.0000  -1.0000   862641\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.355469 0.343750          512          512.0   1.0000   1.0000    75855\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.316406 0.277344         1024         1024.0   1.0000  -1.0000   109746\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.233480 0.233480         2048         2048.0  -1.0000  -1.0000   168490 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.221978 0.210526         4096         4096.0   1.0000   1.0000    55611 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.209890 0.197802         8192         8192.0   1.0000   1.0000   458403 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 6\r\n",
        "weighted example sum = 8640.000000\r\n",
        "weighted label sum = -36.000000\r\n",
        "average loss = 0.187500 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 2951686032\r\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is significantly slower *and* significantly worse, basically because there are now hundreds of thousands of features, and the model has overfit.\n",
      "\n",
      "Minor note: when creating quadratic features, you can use `:` as a wildcard to refer to \"any namespace\", for instance \"`-q l:`\" pairs `l` with all other namespaces; \"`-q ::`\" pairs all namespaces with all other namespaces.\n",
      "\n",
      "# <a id='reg'></a> Regularization\n",
      "\n",
      "Regularization is a sometime-helpful method for preventing your model from overfitting to the training data. Once you have a reasonable amount of data, I find regularization in `vw` to be relatively **un**helpful, largely because the underlying learning algorithm is quite good. But for small or modest data set sizes, like the sentiment data, it is plausibly useful.\n",
      "\n",
      "`vw` has two built-in forms of regularization: $\\ell_2$ (\"Gaussian\") regularization and $\\ell_1$ (\"sparse\") regularization. You can combing them if you want to get \"elastic net\" regularization. Both forms for regularization require a strength parameter, which usually should be quite small and must be tuned carefully. Doing $\\ell_1$ has the advantage of often producing models with lots of zeros. Here are some runs with both, where I've chosen regularization strengths by hand that work well. We're also going to save a readable model to disk so we can look at how many features are being used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l2 0.0001 -f data/sentiment-lex.model\n",
      "!vw -i data/sentiment-lex.model -t --invert_hash data/sentiment-lex.model.readable data/sentiment-lex.tr --quiet\n",
      "!echo \"\"\n",
      "!echo \"total number of features:\"\n",
      "!tail -n+13 data/sentiment-lex.model.readable | wc -l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating quadratic features for pairs: wl \r\n",
        "using l2 regularization = 0.0001\r\n",
        "final_regressor = data/sentiment-lex.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     2220\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1890\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     2610\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1578\r\n",
        "0.625000 0.750000           16           16.0   1.0000   1.0000     1470\r\n",
        "0.437500 0.250000           32           32.0  -1.0000  -1.0000     1362\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.390625 0.343750           64           64.0  -1.0000  -1.0000     1560\r\n",
        "0.367188 0.343750          128          128.0   1.0000  -1.0000     1689\r\n",
        "0.347656 0.328125          256          256.0   1.0000   1.0000     3933\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.324219 0.300781          512          512.0   1.0000   1.0000     1161\r\n",
        "0.276367 0.228516         1024         1024.0   1.0000   1.0000     1398\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.180617 0.180617         2048         2048.0  -1.0000  -1.0000     1734 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.158242 0.135965         4096         4096.0   1.0000   1.0000      993 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.139560 0.120879         8192         8192.0   1.0000   1.0000     2865 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.125824 0.112088        16384        16384.0   1.0000   1.0000     1965 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 12\r\n",
        "weighted example sum = 17280.000000\r\n",
        "weighted label sum = -72.000000\r\n",
        "average loss = 0.100000 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 39013416\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total number of features:\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "130223\r\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this, a regularization strength of 0.0001 has dropped the error rate from 11.3% to 10.0%! And the model is using 130k features.\n",
      "\n",
      "We can also try $\\ell_1$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l1 0.000001 -f data/sentiment-lex.model\n",
      "!vw -i data/sentiment-lex.model -t --invert_hash data/sentiment-lex.model.readable data/sentiment-lex.tr --quiet\n",
      "!echo \"\"\n",
      "!echo \"total number of features:\"\n",
      "!tail -n+13 data/sentiment-lex.model.readable | wc -l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating quadratic features for pairs: wl \r\n",
        "using l1 regularization = 1e-06\r\n",
        "final_regressor = data/sentiment-lex.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     2220\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1890\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     2610\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1578\r\n",
        "0.625000 0.750000           16           16.0   1.0000   1.0000     1470\r\n",
        "0.437500 0.250000           32           32.0  -1.0000  -1.0000     1362\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.390625 0.343750           64           64.0  -1.0000  -1.0000     1560\r\n",
        "0.367188 0.343750          128          128.0   1.0000  -1.0000     1689\r\n",
        "0.347656 0.328125          256          256.0   1.0000   1.0000     3933\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.324219 0.300781          512          512.0   1.0000   1.0000     1161\r\n",
        "0.277344 0.230469         1024         1024.0   1.0000   1.0000     1398\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.180617 0.180617         2048         2048.0  -1.0000  -1.0000     1734 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.156044 0.131579         4096         4096.0   1.0000   1.0000      993 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.141758 0.127473         8192         8192.0   1.0000   1.0000     2865 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 8\r\n",
        "weighted example sum = 11520.000000\r\n",
        "weighted label sum = -48.000000\r\n",
        "average loss = 0.118750 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 26008944\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total number of features:\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "97106\r\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This gives us a slightly higher loss (the original value of 11.9%) but using fewer features: only 97k in this case. We can increase the strength of the regularizer and get fewer features at the cost of higher error:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l1 0.00001 -f data/sentiment-lex.model\n",
      "!vw -i data/sentiment-lex.model -t --invert_hash data/sentiment-lex.model.readable data/sentiment-lex.tr --quiet\n",
      "!echo \"\"\n",
      "!echo \"total number of features:\"\n",
      "!tail -n+13 data/sentiment-lex.model.readable | wc -l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating quadratic features for pairs: wl \r\n",
        "using l1 regularization = 1e-05\r\n",
        "final_regressor = data/sentiment-lex.model\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000     2220\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000     1890\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000     2610\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000     1578\r\n",
        "0.625000 0.750000           16           16.0   1.0000   1.0000     1470\r\n",
        "0.437500 0.250000           32           32.0  -1.0000  -1.0000     1362\r\n",
        "0.390625 0.343750           64           64.0  -1.0000  -1.0000     1560\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.367188 0.343750          128          128.0   1.0000  -1.0000     1689\r\n",
        "0.355469 0.343750          256          256.0   1.0000   1.0000     3933\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.324219 0.292969          512          512.0   1.0000   1.0000     1161\r\n",
        "0.285156 0.246094         1024         1024.0   1.0000   1.0000     1398\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.220264 0.220264         2048         2048.0  -1.0000  -1.0000     1734 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.189011 0.157895         4096         4096.0   1.0000   1.0000      993 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.171429 0.153846         8192         8192.0   1.0000   1.0000     2865 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 6\r\n",
        "weighted example sum = 8640.000000\r\n",
        "weighted label sum = -36.000000\r\n",
        "average loss = 0.150000 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 19506708\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total number of features:\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "25809\r\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This shows that if we're willing to suffer a bit more loss (in this case, 15.0%), we can get a model that's about a quarter of the size.\n",
      "\n",
      "If you add both `--l1` and `--l2` (and tune the two corresponding hyperparameters), you get elastic net. I've never had this be particularly effective.\n",
      "\n",
      "# <a id='nn'></a>  Neural networks\n",
      "\n",
      "We've already seen several ways of achieving non-linearity if `vw`: quadratic features, ngrams, etc. A more traditional approach is to add a hidden layer of representation, yielding a two-layer feed-forward neural network. To achieve this, you simply say `--nn 10`, to get ten hidden layers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function hinge -b 24 --nn 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      742\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      632\r\n",
        "0.500000 0.500000            4            4.0   1.0000   1.0000      872\r\n",
        "0.625000 0.750000            8            8.0  -1.0000   1.0000      528\r\n",
        "0.562500 0.500000           16           16.0   1.0000   1.0000      492\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.562500 0.562500           32           32.0  -1.0000   1.0000      456\r\n",
        "0.578125 0.593750           64           64.0  -1.0000   1.0000      522\r\n",
        "0.515625 0.453125          128          128.0   1.0000   1.0000      565\r\n",
        "0.496094 0.476562          256          256.0   1.0000   1.0000     1313\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.482422 0.468750          512          512.0   1.0000   1.0000      389\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.437500 0.392578         1024         1024.0   1.0000   1.0000      468\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.334802 0.334802         2048         2048.0  -1.0000  -1.0000      580 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.257143 0.179825         4096         4096.0   1.0000   1.0000      333 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.217582 0.178022         8192         8192.0   1.0000   1.0000      957 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 6\r\n",
        "weighted example sum = 8640.000000\r\n",
        "weighted label sum = -36.000000\r\n",
        "average loss = 0.162500 h\r\n",
        "best constant = -1.000000\r\n",
        "best constant's loss = 0.995833\r\n",
        "total feature number = 6520056\r\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Note: I switched to hinge loss. With logistic loss, you get about 50% error which is horrible. This has to do with scaling of the gradients.) Well, that is disappointing. Our loss went from around 12% to around 16%.\n",
      "\n",
      "One reason is that the original bag of words features are actually quite useful, and by forcing them through a 5-unit hidden layer you lose a lot of information. A solution to this problem is `--inpass` which adds additional edges to the neural network directly from the input to the output. So you basically get the best of both worlds:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function hinge -b 24 --nn 10 --inpass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using input passthrough for neural network training\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      742\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      632\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      872\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      528\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      492\r\n",
        "0.593750 0.500000           32           32.0  -1.0000   1.0000      456\r\n",
        "0.484375 0.375000           64           64.0  -1.0000   1.0000      522\r\n",
        "0.398438 0.312500          128          128.0   1.0000   1.0000      565\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.410156 0.421875          256          256.0   1.0000   1.0000     1313\r\n",
        "0.369141 0.328125          512          512.0   1.0000   1.0000      389\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.314453 0.259766         1024         1024.0   1.0000   1.0000      468\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.202643 0.202643         2048         2048.0  -1.0000  -1.0000      580 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.186813 0.171053         4096         4096.0   1.0000   1.0000      333 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.169231 0.151648         8192         8192.0   1.0000   1.0000      957 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 7\r\n",
        "weighted example sum = 10080.000000\r\n",
        "weighted label sum = -42.000000\r\n",
        "average loss = 0.137500 h\r\n",
        "best constant = -1.000000\r\n",
        "best constant's loss = 0.995833\r\n",
        "total feature number = 7606732\r\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, so this is better, we're down to 13.8% error. But what else can we do? **DROPOUT**! This is a particularly useful method of regularization for neural networks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function hinge -b 24 --nn 10 --inpass --dropout"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using dropout for neural network training\r\n",
        "using input passthrough for neural network training\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      742\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      632\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      872\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      528\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      492\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.562500 0.437500           32           32.0  -1.0000   1.0000      456\r\n",
        "0.453125 0.343750           64           64.0  -1.0000  -1.0000      522\r\n",
        "0.382812 0.312500          128          128.0   1.0000   1.0000      565\r\n",
        "0.398438 0.414062          256          256.0   1.0000   1.0000     1313\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.355469 0.312500          512          512.0   1.0000   1.0000      389\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.310547 0.265625         1024         1024.0   1.0000   1.0000      468\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.202643 0.202643         2048         2048.0  -1.0000  -1.0000      580 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.184615 0.166667         4096         4096.0   1.0000   1.0000      333 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.170330 0.156044         8192         8192.0   1.0000   1.0000      957 h\r\n",
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 6\r\n",
        "weighted example sum = 8640.000000\r\n",
        "weighted label sum = -36.000000\r\n",
        "average loss = 0.143750 h\r\n",
        "best constant = -1.000000\r\n",
        "best constant's loss = 0.995833\r\n",
        "total feature number = 6520056\r\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, disappointed once again: loss is back up to 14.4%.\n",
      "\n",
      "By playing around with: (a) loss function, (b) size of the hidden units and (c) dropout, I **was** able to get a model that's slightly better:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw --binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 --nn 10 --inpass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using input passthrough for neural network training\r\n",
        "Num weight bits = 24\r\n",
        "learning rate = 0.5\r\n",
        "initial_t = 0\r\n",
        "power_t = 0.5\r\n",
        "decay_learning_rate = 1\r\n",
        "creating cache_file = data/sentiment-lex.tr.cache\r\n",
        "Reading datafile = data/sentiment-lex.tr\r\n",
        "num sources = 1\r\n",
        "average  since         example        example  current  current  current\r\n",
        "loss     last          counter         weight    label  predict features\r\n",
        "1.000000 1.000000            1            1.0   1.0000  -1.0000      742\r\n",
        "0.500000 0.000000            2            2.0   1.0000   1.0000      632\r\n",
        "0.750000 1.000000            4            4.0   1.0000  -1.0000      872\r\n",
        "0.500000 0.250000            8            8.0  -1.0000  -1.0000      528\r\n",
        "0.687500 0.875000           16           16.0   1.0000  -1.0000      492\r\n",
        "0.468750 0.250000           32           32.0  -1.0000  -1.0000      456\r\n",
        "0.390625 0.312500           64           64.0  -1.0000  -1.0000      522\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.351562 0.312500          128          128.0   1.0000  -1.0000      565\r\n",
        "0.359375 0.367188          256          256.0   1.0000   1.0000     1313\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.337891 0.316406          512          512.0   1.0000   1.0000      389\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.289062 0.240234         1024         1024.0   1.0000  -1.0000      468\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.198238 0.198238         2048         2048.0  -1.0000  -1.0000      580 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.175824 0.153509         4096         4096.0   1.0000   1.0000      333 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.158242 0.140659         8192         8192.0   1.0000   1.0000      957 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.139011 0.119780        16384        16384.0   1.0000   1.0000      657 h\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "finished run\r\n",
        "number of examples per pass = 1440\r\n",
        "passes used = 15\r\n",
        "weighted example sum = 21600.000000\r\n",
        "weighted label sum = -90.000000\r\n",
        "average loss = 0.106250 h\r\n",
        "best constant = -0.008333\r\n",
        "best constant's loss = 0.693139\r\n",
        "total feature number = 16300140\r\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note, here, that we've switched *back* to logistic loss. When you use `--inpass` and get features directly from the input, you don't have the same problems with logistic loss that you do when you don't use `--inpass`. It definitely requires some fiddling.\n",
      "\n",
      "# <a id='summary'></a>Summary\n",
      "\n",
      "In this notebook, we've learned lots of ways to get extra features from `vw`. Here's a brief summary:\n",
      "\n",
      "* Using `-b 24` to increase the size of the model, something you should always do\n",
      "* `--affix +6,-2w` to add six character prefixes to features from all namespaces and two character suffixes to features from the w namespace\n",
      "* `--spelling w` to add spelling features to the w namespace (use `--spelling _`) to add spelling features to the default namespace\n",
      "* `--ngram 3 --skips 1` to add one-skip, trigram features to all namespaces\n",
      "* `--loss_function logistic/hinge` to switch the loss function\n",
      "* How to get a human readable model using `--invert_hash`\n",
      "* Using `--early_terminate 999 --holdout_after 1401` to treat the last 200 examples as development data and turn off early stopping\n",
      "* Using namespaces and `-q` for quadratic features (there's also `--cubic` for cubic features!)\n",
      "* Using `--l2` or `--l1` to regularize the model\n",
      "* Running in neural networks mode with `--nn 10 --inpass --dropout`, and then playing around with other parameters\n",
      "\n",
      "One important thing to remember is that arguments that affect the features that `vw` use get stored in saved models. This means that if you train with `-f model` and then test with `-t -i model`, when you load the model (`-i model`), you *also* load all of the feature generators. This ensures that training and testing use a consistent feature representation, and also means you don't have to remember what arguments you used to train the model."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}